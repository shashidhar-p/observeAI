# ==============================================================================
# Multi-Agent RCA System Configuration
# ==============================================================================

# ==============================================================================
# LLM Provider Configuration
# ==============================================================================
# Choose your LLM provider: 'openai', 'gemini', 'ollama' (local)
LLM_PROVIDER=ollama

# --- OpenAI Configuration (if LLM_PROVIDER=openai) ---
# Get your API key from https://platform.openai.com/
OPENAI_API_KEY=sk-your-key-here
# Model to use (gpt-4o, gpt-4-turbo, gpt-3.5-turbo, etc.)
OPENAI_MODEL=gpt-4o

# --- Gemini Configuration (if LLM_PROVIDER=gemini) ---
# Get your API key from https://aistudio.google.com/
GEMINI_API_KEY=your-key-here
# Model to use (gemini-1.5-pro, gemini-1.5-flash, etc.)
GEMINI_MODEL=gemini-1.5-pro

# --- Ollama Configuration (if LLM_PROVIDER=ollama) ---
# Ollama server URL (run 'ollama serve' to start)
OLLAMA_BASE_URL=http://localhost:11434
# Model to use (run 'ollama pull llama3.1:8b' to download)
# Recommended models with tool calling: llama3.1:8b, mistral:7b, mixtral:8x7b
OLLAMA_MODEL=llama3.1:8b
# Timeout for Ollama requests (local models can be slower)
OLLAMA_TIMEOUT_SECONDS=300

# Database (Required)
# PostgreSQL connection URL with asyncpg driver
DATABASE_URL=postgresql+asyncpg://rca:rca@localhost:5432/rca_db

# Observability Backends (Required)
# Loki for logs (LogQL queries)
LOKI_URL=http://localhost:3100
# Cortex for metrics (PromQL queries)
CORTEX_URL=http://localhost:9009

# Server Settings
HOST=0.0.0.0
PORT=8000
DEBUG=true

# RCA Configuration
# Time window (seconds) for grouping related alerts into incidents
CORRELATION_WINDOW_SECONDS=300
# Maximum iterations for the RCA agent loop (prevents infinite loops)
RCA_MAX_ITERATIONS=10
# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# RCA Expert Context (Optional)
# Custom domain expertise to inject into the RCA agent's system prompt.
# Leave empty to use the default network engineer context.
# For multi-line values, use \n for newlines or set via code.
# Example: RCA_EXPERT_CONTEXT="You are a Kubernetes expert. Focus on pod scheduling issues..."
RCA_EXPERT_CONTEXT=

# Timeouts (seconds)
# Timeout for Loki queries
LOKI_TIMEOUT_SECONDS=30
# Timeout for Cortex queries
CORTEX_TIMEOUT_SECONDS=30
# Timeout for LLM API calls
LLM_TIMEOUT_SECONDS=120
